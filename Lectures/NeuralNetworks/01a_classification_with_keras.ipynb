{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Course, Bogotá, Colombia  (&copy; Josh Bloom; June 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Keras\n",
    "\n",
    "We just saw how to do regression problems with neural nets in `keras`. Let's now explore classification. Because we'll use this dataset later, let's introduce the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist#labels) dataset: 70k small (28$\\times$28) images of 10 different types of clothing.\n",
    "\n",
    "<img src=\"https://github.com/zalandoresearch/fashion-mnist/blob/master/doc/img/fashion-mnist-sprite.png?raw=true\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each training and test example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "Tensorflow has a simple method to get this data locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, os\n",
    "\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Print keras version\n",
    "print(tensorflow.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # scale the images to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ind = 1\n",
    "plt.axis('off')\n",
    "plt.imshow(x_train[ind], cmap=plt.cm.gray_r, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's treat every pixel as a separate input and build a keras NN to predict the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train[0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... nn_results/colombia_nn_2019-05-21T16:49.h5\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "59712/60000 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9158\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.88210, saving model to nn_results/colombia_nn_2019-05-21T16:49.h5\n",
      "60000/60000 [==============================] - 6s 100us/sample - loss: 0.2266 - accuracy: 0.9158 - val_loss: 0.3481 - val_accuracy: 0.8821\n",
      "Epoch 2/20\n",
      "59520/60000 [============================>.] - ETA: 0s - loss: 0.2215 - accuracy: 0.9174\n",
      "Epoch 00002: val_accuracy improved from 0.88210 to 0.88270, saving model to nn_results/colombia_nn_2019-05-21T16:49.h5\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.2216 - accuracy: 0.9173 - val_loss: 0.3594 - val_accuracy: 0.8827\n",
      "Epoch 3/20\n",
      "59520/60000 [============================>.] - ETA: 0s - loss: 0.2161 - accuracy: 0.9200\n",
      "Epoch 00003: val_accuracy did not improve from 0.88270\n",
      "60000/60000 [==============================] - 5s 92us/sample - loss: 0.2160 - accuracy: 0.9199 - val_loss: 0.3615 - val_accuracy: 0.8824\n",
      "Epoch 4/20\n",
      "59552/60000 [============================>.] - ETA: 0s - loss: 0.2145 - accuracy: 0.9206\n",
      "Epoch 00004: val_accuracy improved from 0.88270 to 0.89330, saving model to nn_results/colombia_nn_2019-05-21T16:49.h5\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2144 - accuracy: 0.9207 - val_loss: 0.3432 - val_accuracy: 0.8933\n",
      "Epoch 5/20\n",
      "59488/60000 [============================>.] - ETA: 0s - loss: 0.2071 - accuracy: 0.9225\n",
      "Epoch 00005: val_accuracy did not improve from 0.89330\n",
      "60000/60000 [==============================] - 5s 87us/sample - loss: 0.2069 - accuracy: 0.9226 - val_loss: 0.3688 - val_accuracy: 0.8863\n",
      "Epoch 6/20\n",
      "59296/60000 [============================>.] - ETA: 0s - loss: 0.2021 - accuracy: 0.9241\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.0007500000356230885.\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.89330\n",
      "60000/60000 [==============================] - 5s 89us/sample - loss: 0.2023 - accuracy: 0.9239 - val_loss: 0.3513 - val_accuracy: 0.8888\n",
      "Epoch 7/20\n",
      "59840/60000 [============================>.] - ETA: 0s - loss: 0.1850 - accuracy: 0.9307\n",
      "Epoch 00007: val_accuracy did not improve from 0.89330\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.1852 - accuracy: 0.9307 - val_loss: 0.3706 - val_accuracy: 0.8919\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0xb360a1908>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "# define path to save model\n",
    "model_path = f'nn_results/colombia_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.75,\n",
    "                              patience=2, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, \n",
    "                                             patience=3, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=20, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice above that the accuracy is much higher than the val_accuracy. That is, we overfit on the training data. One way to help protect against this is to introduce `Dropout`\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*iWQzxhVlvadk6VAJjsgXgg.png\">\n",
    "\n",
    "Srivastava, Nitish, et al. ”Dropout: a simple way to prevent neural networks from\n",
    "overfitting”, JMLR 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=input_shape))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))  # 20% chance of dropping a node during training\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "# define path to save model\n",
    "model_path = f'nn_results/colombia_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "logdir = os.path.join(\"nn_results\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.75,\n",
    "                              patience=2, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = tf.keras.callbacks.CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.001, \n",
    "                                             patience=3, \\\n",
    "                                             verbose=1, mode='auto')\n",
    "\n",
    "model_check = tf.keras.callbacks.ModelCheckpoint(model_path,\n",
    "        monitor='val_accuracy', \n",
    "        save_best_only=True, \n",
    "        mode='max',\n",
    "        verbose=1)\n",
    "\n",
    "model.fit(x=x_train, \n",
    "          y=y_train, \n",
    "          epochs=20, \n",
    "          validation_data=(x_test, y_test), \n",
    "          callbacks=[tensorboard_callback, reduce_lr, csv_logger, earlystop, model_check])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "conf_mat_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(conf_mat_normalized)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = {0: \"T-shirt/top\",\n",
    "          1: \"Trouser\",\n",
    "          2: \"Pullover\",\n",
    "          3: \"Dress\",\n",
    "          4: \"Coat\",\n",
    "          5: \"Sandal\",\n",
    "          6: \"Shirt\",\n",
    "          7: \"Sneaker\",\n",
    "          8: \"Bag\",\n",
    "          9: \"Ankle boot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_wrong = []\n",
    "for i, (pred, actual) in enumerate(zip(model.predict_classes(x_test),y_test)):\n",
    "    if pred != actual:\n",
    "        ind_wrong.append((i,pred, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 100\n",
    "plt.imshow(x_test[ind_wrong[ind][0]], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"pred={lookup[ind_wrong[ind][1]]} true={lookup[ind_wrong[ind][2]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How'd we do? The creators of the dataset maintain a website with results using `sklearn`:\n",
    "\n",
    "http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
