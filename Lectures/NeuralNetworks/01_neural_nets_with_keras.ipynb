{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML Course, Bogotá, Colombia  (&copy; Josh Bloom; June 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "\n",
       ".rendered_html\n",
       "{\n",
       "  color: #2C5494;\n",
       "  font-family: Ubuntu;\n",
       "  font-size: 140%;\n",
       "  line-height: 1.1;\n",
       "  margin: 0.5em 0;\n",
       "  }\n",
       "\n",
       ".talk_title\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 250%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 10px 50px 10px;\n",
       "  }\n",
       "\n",
       ".subtitle\n",
       "{\n",
       "  color: #386BBC;\n",
       "  font-size: 180%;\n",
       "  font-weight:bold;\n",
       "  line-height: 1.2; \n",
       "  margin: 20px 50px 20px;\n",
       "  }\n",
       "\n",
       ".slide-header, p.slide-header\n",
       "{\n",
       "  color: #498AF3;\n",
       "  font-size: 200%;\n",
       "  font-weight:bold;\n",
       "  margin: 0px 20px 10px;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".rendered_html h1\n",
       "{\n",
       "  color: #498AF3;\n",
       "  line-height: 1.2; \n",
       "  margin: 0.15em 0em 0.5em;\n",
       "  page-break-before: always;\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       "\n",
       ".rendered_html h2\n",
       "{ \n",
       "  color: #386BBC;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html h3\n",
       "{ \n",
       "  font-size: 100%;\n",
       "  line-height: 1.2;\n",
       "  margin: 1.1em 0em 0.5em;\n",
       "  }\n",
       "\n",
       ".rendered_html li\n",
       "{\n",
       "  line-height: 1.8;\n",
       "  }\n",
       "\n",
       ".input_prompt, .CodeMirror-lines, .output_area\n",
       "{\n",
       "  font-family: Consolas;\n",
       "  font-size: 120%;\n",
       "  }\n",
       "\n",
       ".gap-above\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap01\n",
       "{\n",
       "  padding-top: 10px;\n",
       "  }\n",
       "\n",
       ".gap05\n",
       "{\n",
       "  padding-top: 50px;\n",
       "  }\n",
       "\n",
       ".gap1\n",
       "{\n",
       "  padding-top: 100px;\n",
       "  }\n",
       "\n",
       ".gap2\n",
       "{\n",
       "  padding-top: 200px;\n",
       "  }\n",
       "\n",
       ".gap3\n",
       "{\n",
       "  padding-top: 300px;\n",
       "  }\n",
       "\n",
       ".emph\n",
       "{\n",
       "  color: #386BBC;\n",
       "  }\n",
       "\n",
       ".warn\n",
       "{\n",
       "  color: red;\n",
       "  }\n",
       "\n",
       ".center\n",
       "{\n",
       "  text-align: center;\n",
       "  }\n",
       "\n",
       ".nb_link\n",
       "{\n",
       "    padding-bottom: 0.5em;\n",
       "}\n",
       "\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run ../talktools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">sklearn is not built for deep/complex networks such as required in covnets (as we'll see later on). We must go to specialized software (and potentially specialized hardware)</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Deep Learning Frameworks\n",
    "\n",
    "Almost all frameworks written in low-level C++/C with Python (or other scripting bindings)\n",
    "\n",
    "### Low-level frameworks\n",
    "\n",
    "   - Tensorflow (Google) Nov 2015\n",
    "   - Theano\n",
    "   - Caffe (Berkeley)\n",
    "   - Torch (Lua)\n",
    "   - pytorch (Python)\n",
    "   - CNTK (Microsoft)\n",
    "   - Chainer\n",
    "   - PaddlePaddle (Baidu) Aug 2016\n",
    "   \n",
    "### High level frameworks (Python)\n",
    "\n",
    "   - Keras (atop Tensorflow, Theano)\n",
    "   - TFLearn \n",
    "   - nolearn\n",
    "   - SkFlow (part of tensorflow)\n",
    "   - [Lasagne](http://lasagne.readthedocs.io/en/latest/index.html) (atop Theano)\n",
    "   \n",
    "<img src=\"https://pbs.twimg.com/media/DX0lfBNU8AEs8KG.png:large\" width=\"75%\">\n",
    "Source: https://twitter.com/fchollet/status/971863128341323776\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">TensorFlow is the platform of choice for deep learning in the research community. These are deep learning framework mentions on arXiv over the past 3 months <img src=\"https://pbs.twimg.com/media/DXy_uc0VAAAIhKG.jpg:small\">\n",
    "\n",
    "&mdash; François Chollet (@fchollet) <a href=\"https://twitter.com/fchollet/status/971863128341323776?ref_src=twsrc%5Etfw\">March 8, 2018</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "\n",
    "see also: https://github.com/mbadry1/Top-Deep-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load up the California housing data as in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "\n",
    "cal_house = datasets.california_housing\n",
    "cal_data = cal_house.fetch_california_housing()\n",
    "X = cal_data['data']   # 8 features \n",
    "Y = cal_data['target'] # response (median house price)\n",
    "\n",
    "half = math.floor(len(Y)/2)\n",
    "train_X = X[:half]\n",
    "train_Y = Y[:half]\n",
    "test_X = X[half:]\n",
    "test_Y = Y[half:]\n",
    "\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test data\n",
    "test_X = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input_features = train_X.shape[1]\n",
    "print(f'number of input features = {num_input_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential  # linear stack of layers\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Print TensorFlow version\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_clf():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_shape=(num_input_features,), \n",
    "                      activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(10,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(5,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "    model.add(Dense(1, activation=\"linear\", kernel_initializer='random_uniform'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae',\"mse\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn_clf()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(\"model_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "clf = KerasRegressor(build_fn=nn_clf, batch_size=32, epochs=50)\n",
    "clf.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well did we do?\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_Y, clf.predict(test_X)) ; print(\"MSE\",mse)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"NN Regression Residuals - MSE = %.3f\" % mse)\n",
    "plt.scatter(test_Y,clf.predict(test_X),alpha=0.4,s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2,5],[0.2,5],c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complete Example\n",
    "\n",
    "We want to train and make some decisions of when to stop based on `validation` data. Ultimately, we'd like to see how well our model would do on truly new data (`test`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage, valid_percentage, test_percentage = (0.90,0.05,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rnd = np.random.RandomState(42)\n",
    "\n",
    "# make an array with the indices of all the rows in the dataset\n",
    "ind_arr = np.arange(X.shape[0])\n",
    "rnd.shuffle(ind_arr)\n",
    "\n",
    "train_ind, tmp = train_test_split(ind_arr, train_size=train_percentage, random_state=rnd)\n",
    "valid_ind, test_ind = train_test_split(tmp, \n",
    "                                                          train_size=valid_percentage/(valid_percentage + test_percentage), \n",
    "                                                          random_state=rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that we're getting all the indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.all(np.array(sorted(list(train_ind) + list(valid_ind) + list(test_ind)))  == sorted(ind_arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's scale the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()  \n",
    "\n",
    "train_X = X[train_ind]\n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(train_X)  \n",
    "train_X = scaler.transform(train_X)  \n",
    "\n",
    "# apply same transformation to test, validation data\n",
    "test_X = scaler.transform(X[test_ind])\n",
    "valid_X = scaler.transform(X[valid_ind])\n",
    "\n",
    "train_y = Y[train_ind] ; test_y = Y[test_ind] ; valid_y = Y[valid_ind]\n",
    "\n",
    "assert train_y.shape[0] == train_X.shape[0]\n",
    "assert test_y.shape[0] == test_X.shape[0]\n",
    "assert valid_y.shape[0] == valid_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this little script (`reproduce.py`) to reset the random seeds, making the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run reproduce.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "model.add(Dense(10, input_shape=(num_input_features,), \n",
    "                  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "model.add(Dense(10,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "model.add(Dense(5,  activation=\"relu\", kernel_initializer='random_uniform'))\n",
    "model.add(Dense(1, activation=\"linear\", kernel_initializer='random_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model.compile` method in `keras` has a number of input parameters:\n",
    "\n",
    "```python\n",
    "compile(optimizer, loss=None, metrics=None, loss_weights=None, sample_weight_mode=None, weighted_metrics=None, target_tensors=None)\n",
    "```\n",
    "Usually, you'll set the `optimizer`, `loss`, and `metrics`.\n",
    "\n",
    "https://keras.io/models/model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error',  metrics=['mae',\"mse\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "from keras.callbacks import EarlyStopping, CSVLogger, \\\n",
    "                                                ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "try:\n",
    "    os.mkdir('nn_results')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "batch_size=64\n",
    "num_epochs = 200\n",
    "\n",
    "run_time_string = datetime.datetime.utcnow().isoformat(timespec='minutes')\n",
    "\n",
    "# define path to save model\n",
    "model_path = f'nn_results/colombia_nn_{run_time_string}.h5'\n",
    "print(f\"Training ... {model_path}\")\n",
    "\n",
    "# Tensorboard is a project which can ingest learning logs for interactive display...more on that later.\n",
    "tb = TensorBoard(log_dir='nn_results', histogram_freq=0, batch_size=batch_size, \n",
    "                 write_graph=True, \n",
    "                 write_grads=False, \n",
    "                 write_images=False, \n",
    "                 embeddings_freq=0, \n",
    "                 embeddings_layer_names=None, \n",
    "                 embeddings_metadata=None, embeddings_data=None)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_mean_squared_error', factor=0.75,\n",
    "                              patience=3, min_lr=1e-6, verbose=1, cooldown=0)\n",
    "\n",
    "csv_logger = CSVLogger(f'nn_results/training_{run_time_string}.log')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_mean_squared_error', min_delta=0.001, patience=10, \\\n",
    "                          verbose=1, mode='auto')\n",
    "\n",
    "model_check = ModelCheckpoint(model_path,\n",
    "        monitor='val_mean_squared_error', \n",
    "        save_best_only=True, \n",
    "        mode='min',\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Model.fit` method:\n",
    "\n",
    "```python\n",
    "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_freq=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_X, y=train_y,\n",
    "               epochs=num_epochs,\n",
    "               validation_data=(valid_X, valid_y),\n",
    "               verbose=1, shuffle=True,\n",
    "               callbacks=[csv_logger, earlystop, model_check, tb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls nn_results/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the history of the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastest_log_file = !ls -t1 nn_results/training* | head -1\n",
    "hist_df = pd.read_csv(lastest_log_file[0])\n",
    "hist_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also available in the return value from `.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"poster\")\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.title(\"Training History\")\n",
    "plt.plot(hist_df.index + 1,hist_df[\"val_mean_squared_error\"] ,alpha=0.4, label=\"validation\")\n",
    "plt.plot(hist_df.index + 1,hist_df[\"mean_squared_error\"] ,alpha=0.4, label=\"training\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.loglog()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `tensorboard` notebook as well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the best model\n",
    "from keras.models import load_model\n",
    "import tensorflow as tf\n",
    "\n",
    "model = load_model(\"nn_results/colombia_nn_2019-05-21T03:26.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_X)\n",
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how well did we do?\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "mse = mean_squared_error(test_y, pred_y); print(\"MSE\",mse)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title(\"NN Regression Residuals - MSE = %.3f\" % mse)\n",
    "plt.scatter(test_y,pred_y ,alpha=0.4,s=3)\n",
    "plt.xlabel(\"Test Y\")\n",
    "plt.ylabel(\"Predicted Y\")\n",
    "plt.plot([0.2,5],[0.2,5],c=\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do compared to the validation and training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
